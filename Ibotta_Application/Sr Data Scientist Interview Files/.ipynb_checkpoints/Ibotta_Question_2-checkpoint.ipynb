{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ibotta_Question_1.ipynb             \u001b[31mcustomers.csv\u001b[m\u001b[m*\r\n",
      "Ibotta_Question_2.ipynb             item_sample.csv\r\n",
      "Test Collaborative Filtering.ipynb  \u001b[31mitem_sample.zip\u001b[m\u001b[m*\r\n",
      "Test_PCA.ipynb                      lat_long_data.csv\r\n",
      "\u001b[31mbrands.csv\u001b[m\u001b[m*                         \u001b[31mretailers.csv\u001b[m\u001b[m*\r\n",
      "\u001b[31mcategories.csv\u001b[m\u001b[m*                     top_related_sprite_customers.csv\r\n"
     ]
    }
   ],
   "source": [
    "ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import scipy.sparse as sparse\n",
    "import numpy as np\n",
    "from scipy.sparse.linalg import spsolve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ibotta_Question_1.ipynb             \u001b[31mcustomers.csv\u001b[m\u001b[m*\r\n",
      "Ibotta_Question_2.ipynb             item_sample.csv\r\n",
      "Test Collaborative Filtering.ipynb  \u001b[31mitem_sample.zip\u001b[m\u001b[m*\r\n",
      "Test_PCA.ipynb                      lat_long_data.csv\r\n",
      "\u001b[31mbrands.csv\u001b[m\u001b[m*                         \u001b[31mretailers.csv\u001b[m\u001b[m*\r\n",
      "\u001b[31mcategories.csv\u001b[m\u001b[m*                     top_related_sprite_customers.csv\r\n"
     ]
    }
   ],
   "source": [
    "ls"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ibotta Test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A large number of product redemption offers are available through our mobile app to consumers. However, consumers do not see the majority of these offers. We need to assist consumers discover relevant offers.\n",
    "\n",
    "Create a machine learning system which  recommends the top 10 product IDs for each user that they have not previously purchased, based on prior purchase behavior for that consumer.\n",
    "\n",
    "● List any assumptions that you make, and provide clarity on the methodology you employ.\n",
    "● Submit any details/documentation that will support your analysis.\n",
    "● Identify additional data you would want before implementing the system into production.\n",
    "● Provide guidance for implementation of the recommendation system.\n",
    "● Provide any underlying analysis where possible, and present back your findings and a summary of\n",
    "what you did and learned."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Pull in item list and drop nulls\n",
    "cleaned_retail = pd.read_csv(\"item_sample.csv\").dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>customer_id</th>\n",
       "      <th>purchase_timestamp</th>\n",
       "      <th>category_id</th>\n",
       "      <th>brand_id</th>\n",
       "      <th>product_id</th>\n",
       "      <th>retailer_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>4995150</td>\n",
       "      <td>2016-05-19 10:42:06</td>\n",
       "      <td>71</td>\n",
       "      <td>3360</td>\n",
       "      <td>170212</td>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4995150</td>\n",
       "      <td>2016-05-19 10:42:06</td>\n",
       "      <td>282</td>\n",
       "      <td>14043</td>\n",
       "      <td>438259</td>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4995150</td>\n",
       "      <td>2016-05-19 10:42:06</td>\n",
       "      <td>282</td>\n",
       "      <td>13652</td>\n",
       "      <td>438932</td>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4995150</td>\n",
       "      <td>2016-05-19 10:42:06</td>\n",
       "      <td>72</td>\n",
       "      <td>433</td>\n",
       "      <td>768289</td>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>8053153</td>\n",
       "      <td>2016-05-19 14:13:08</td>\n",
       "      <td>220</td>\n",
       "      <td>24854</td>\n",
       "      <td>388549</td>\n",
       "      <td>18</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   customer_id   purchase_timestamp  category_id  brand_id  product_id  \\\n",
       "0      4995150  2016-05-19 10:42:06           71      3360      170212   \n",
       "1      4995150  2016-05-19 10:42:06          282     14043      438259   \n",
       "2      4995150  2016-05-19 10:42:06          282     13652      438932   \n",
       "3      4995150  2016-05-19 10:42:06           72       433      768289   \n",
       "4      8053153  2016-05-19 14:13:08          220     24854      388549   \n",
       "\n",
       "   retailer_id  \n",
       "0           12  \n",
       "1           12  \n",
       "2           12  \n",
       "3           12  \n",
       "4           18  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cleaned_retail.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Create an item lookup to use later for the recommender system\n",
    "brand_product = cleaned_retail[['brand_id','product_id']]\n",
    "item_lookup = pd.read_csv(\"brands.csv\")\n",
    "item_lookup.columns = ['brand_id','brand_name']\n",
    "item_lookup = item_lookup.merge(brand_product, on='brand_id')[['product_id','brand_name' ]].drop_duplicates()\n",
    "item_lookup['product_id'] = item_lookup.product_id.astype(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>customer_id</th>\n",
       "      <th>product_id</th>\n",
       "      <th>quantity</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>156</td>\n",
       "      <td>8119</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>156</td>\n",
       "      <td>8120</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>156</td>\n",
       "      <td>8121</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>156</td>\n",
       "      <td>150209</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>156</td>\n",
       "      <td>174738</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   customer_id  product_id  quantity\n",
       "0          156        8119         2\n",
       "1          156        8120         1\n",
       "2          156        8121         1\n",
       "3          156      150209         2\n",
       "4          156      174738         1"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Aggregate quantity bought by each customer\n",
    "grouped_purchased = cleaned_retail.groupby(['customer_id', 'product_id'])['purchase_timestamp'].count().reset_index()\n",
    "grouped_purchased.columns = [\"customer_id\", \"product_id\", \"quantity\"]\n",
    "grouped_purchased.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Instead of representing an explicit rating, the purchase quantity can represent a “confidence” in terms of how strong the interaction was. Items with a larger number of purchases by a customer can carry more weight in our ratings matrix of purchases.\n",
    "\n",
    "Our last step is to create the sparse ratings matrix of users and items utilizing the code below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "customers = list(np.sort(grouped_purchased.customer_id.unique())) # Get our unique customers\n",
    "products = list(np.sort(grouped_purchased.product_id.unique())) # Get our unique products that were purchased\n",
    "quantity = list(np.sort(grouped_purchased.quantity)) # All of our purchases\n",
    "\n",
    "rows = grouped_purchased.customer_id.astype('category', categories = customers).cat.codes \n",
    "\n",
    "# Get the associated row indices\n",
    "cols = grouped_purchased.product_id.astype('category', categories = products).cat.codes\n",
    "\n",
    "# Get the associated column indices\n",
    "purchases_sparse = sparse.csr_matrix((quantity, (rows, cols)), shape=(len(customers), len(products)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<27055x114871 sparse matrix of type '<class 'numpy.int64'>'\n",
       "\twith 2239532 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check out sparse matrix object\n",
    "purchases_sparse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "customers: 27055 products: 114871\n",
      "y: 27055 x: 114871\n"
     ]
    }
   ],
   "source": [
    "print (\"customers:\", len(customers), \"products:\", len(products))\n",
    "print (\"y:\", len(purchases_sparse.getnnz(1)), \"x:\", len(purchases_sparse.getnnz(0)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "99.9279391580165"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check the sparsity of the matrix\n",
    "matrix_size = purchases_sparse.shape[0]*purchases_sparse.shape[1] # Number of possible interactions in the matrix\n",
    "num_purchases = len(purchases_sparse.nonzero()[0]) # Number of items interacted with\n",
    "sparsity = 100*(1 - (num_purchases/matrix_size))\n",
    "sparsity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "99.9% of the interaction matrix is sparse. For collaborative filtering to work, the maximum sparsity you could get away with would probably be about 99.5% or so. We are above this, so we need to reduce the sparsity. \n",
    "\n",
    "Luckily we can easily remove rows and columns that have very low total counts.   Essentially removing low transcation items, and consumers. This is also related to \"cold starting\", a term used to described users without transaction history.  There are multiple ways of dealing with this within an application."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "99.41163625991679"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "purchases_sparse = purchases_sparse[purchases_sparse.getnnz(1)>30][:,purchases_sparse.getnnz(0)>30]\n",
    "\n",
    "matrix_size = purchases_sparse.shape[0]*purchases_sparse.shape[1] # Number of possible interactions in the matrix\n",
    "num_purchases = len(purchases_sparse.nonzero()[0]) # Number of items interacted with\n",
    "sparsity = 100*(1 - (num_purchases/matrix_size))\n",
    "sparsity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([ True,  True,  True, ...,  True, False,  True], dtype=bool)]"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[purchases_sparse.getnnz(1)>30]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "y: 15317 x: 16077\n"
     ]
    }
   ],
   "source": [
    "print (\"y:\", len(purchases_sparse.getnnz(1)), \"x:\", len(purchases_sparse.getnnz(0)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We chose to reduce the sparsity to around 99.3% just to provide a buffer between 99.5.\n",
    "This new sparsity should work fine. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating a Training and Validation Set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "Typically in Machine Learning applications, we need to test whether the model we just trained is any good on new data it hasn’t yet seen before from the training phase.\n",
    "\n",
    "With collaborative filtering, that’s not going to work because you need all of the user/item interactions to find the proper matrix factorization. A better method is to hide a certain percentage of the user/item interactions from the model during the training phase chosen at random. Then, check during the test phase how many of the items that were recommended the user actually ended up purchasing in the end.\n",
    "\n",
    "Ideally, you would ultimately test your recommendations with some kind of A/B test or utilizing data from a time series where all data prior to a certain point in time is used for training while data after a certain period of time is used for testing.\n",
    "\n",
    "We will split "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import random\n",
    "def make_train(ratings, pct_test = 0.2):\n",
    "    '''\n",
    "    This function will take in the original user-item matrix and \"mask\" a percentage of the original ratings where a\n",
    "    user-item interaction has taken place for use as a test set. The test set will contain all of the original ratings, \n",
    "    while the training set replaces the specified percentage of them with a zero in the original ratings matrix. \n",
    "    \n",
    "    parameters: \n",
    "    \n",
    "    ratings - the original ratings matrix from which you want to generate a train/test set. Test is just a complete\n",
    "    copy of the original set. This is in the form of a sparse csr_matrix. \n",
    "    \n",
    "    pct_test - The percentage of user-item interactions where an interaction took place that you want to mask in the \n",
    "    training set for later comparison to the test set, which contains all of the original ratings. \n",
    "    \n",
    "    returns:\n",
    "    \n",
    "    training_set - The altered version of the original data with a certain percentage of the user-item pairs \n",
    "    that originally had interaction set back to zero.\n",
    "    \n",
    "    test_set - A copy of the original ratings matrix, unaltered, so it can be used to see how the rank order \n",
    "    compares with the actual interactions.\n",
    "    \n",
    "    user_inds - From the randomly selected user-item indices, which user rows were altered in the training data.\n",
    "    This will be necessary later when evaluating the performance via AUC.\n",
    "    '''\n",
    "    test_set = ratings.copy() # Make a copy of the original set to be the test set. \n",
    "    test_set[test_set != 0] = 1 # Store the test set as a binary preference matrix\n",
    "    training_set = ratings.copy() # Make a copy of the original data we can alter as our training set. \n",
    "    nonzero_inds = training_set.nonzero() # Find the indices in the ratings data where an interaction exists\n",
    "    nonzero_pairs = list(zip(nonzero_inds[0], nonzero_inds[1])) # Zip these pairs together of user,item index into list\n",
    "    random.seed(0) # Set the random seed to zero for reproducibility\n",
    "    num_samples = int(np.ceil(pct_test*len(nonzero_pairs))) # Round the number of samples needed to the nearest integer\n",
    "    samples = random.sample(nonzero_pairs, num_samples) # Sample a random number of user-item pairs without replacement\n",
    "    user_inds = [index[0] for index in samples] # Get the user row indices\n",
    "    item_inds = [index[1] for index in samples] # Get the item column indices\n",
    "    training_set[user_inds, item_inds] = 0 # Assign all of the randomly chosen user-item pairs to zero\n",
    "    training_set.eliminate_zeros() # Get rid of zeros in sparse array storage after update to save space\n",
    "    return training_set, test_set, list(set(user_inds)) # Output the unique list of user rows that were altered  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "product_train, product_test, product_users_altered = make_train(purchases_sparse, pct_test = 0.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have our train/test split, it is time to implement the alternating least squares algorithm from the Hu, Koren, and Volinsky paper."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def implicit_weighted_ALS(training_set, lambda_val = 0.1, alpha = 40, iterations = 10, rank_size = 20, seed = 0):\n",
    "    '''\n",
    "    Implicit weighted ALS taken from Hu, Koren, and Volinsky 2008. Designed for alternating least squares and implicit\n",
    "    feedback based collaborative filtering. \n",
    "    \n",
    "    parameters:\n",
    "    \n",
    "    training_set - Our matrix of ratings with shape m x n, where m is the number of users and n is the number of items.\n",
    "    Should be a sparse csr matrix to save space. \n",
    "    \n",
    "    lambda_val - Used for regularization during alternating least squares. Increasing this value may increase bias\n",
    "    but decrease variance. Default is 0.1. \n",
    "    \n",
    "    alpha - The parameter associated with the confidence matrix discussed in the paper, where Cui = 1 + alpha*Rui. \n",
    "    The paper found a default of 40 most effective. Decreasing this will decrease the variability in confidence between\n",
    "    various ratings.\n",
    "    \n",
    "    iterations - The number of times to alternate between both user feature vector and item feature vector in\n",
    "    alternating least squares. More iterations will allow better convergence at the cost of increased computation. \n",
    "    The authors found 10 iterations was sufficient, but more may be required to converge. \n",
    "    \n",
    "    rank_size - The number of latent features in the user/item feature vectors. The paper recommends varying this \n",
    "    between 20-200. Increasing the number of features may overfit but could reduce bias. \n",
    "    \n",
    "    seed - Set the seed for reproducible results\n",
    "    \n",
    "    returns:\n",
    "    \n",
    "    The feature vectors for users and items. The dot product of these feature vectors should give you the expected \n",
    "    \"rating\" at each point in your original matrix. \n",
    "    '''\n",
    "    \n",
    "    # first set up our confidence matrix\n",
    "    \n",
    "    conf = (alpha*training_set) # To allow the matrix to stay sparse, I will add one later when each row is taken \n",
    "                                # and converted to dense. \n",
    "    num_user = conf.shape[0]\n",
    "    num_item = conf.shape[1] # Get the size of our original ratings matrix, m x n\n",
    "    \n",
    "    # initialize our X/Y feature vectors randomly with a set seed\n",
    "    rstate = np.random.RandomState(seed)\n",
    "    \n",
    "    X = sparse.csr_matrix(rstate.normal(size = (num_user, rank_size))) # Random numbers in a m x rank shape\n",
    "    Y = sparse.csr_matrix(rstate.normal(size = (num_item, rank_size))) # Normally this would be rank x n but we can \n",
    "                                                                 # transpose at the end. Makes calculation more simple.\n",
    "    X_eye = sparse.eye(num_user)\n",
    "    Y_eye = sparse.eye(num_item)\n",
    "    lambda_eye = lambda_val * sparse.eye(rank_size) # Our regularization term lambda*I. \n",
    "    \n",
    "    # We can compute this before iteration starts. \n",
    "    \n",
    "    # Begin iterations\n",
    "   \n",
    "    for iter_step in range(iterations): # Iterate back and forth between solving X given fixed Y and vice versa\n",
    "        # Compute yTy and xTx at beginning of each iteration to save computing time\n",
    "        yTy = Y.T.dot(Y)\n",
    "        xTx = X.T.dot(X)\n",
    "        # Being iteration to solve for X based on fixed Y\n",
    "        for u in range(num_user):\n",
    "            conf_samp = conf[u,:].toarray() # Grab user row from confidence matrix and convert to dense\n",
    "            pref = conf_samp.copy() \n",
    "            pref[pref != 0] = 1 # Create binarized preference vector \n",
    "            CuI = sparse.diags(conf_samp, [0]) # Get Cu - I term, don't need to subtract 1 since we never added it \n",
    "            yTCuIY = Y.T.dot(CuI).dot(Y) # This is the yT(Cu-I)Y term \n",
    "            yTCupu = Y.T.dot(CuI + Y_eye).dot(pref.T) # This is the yTCuPu term, where we add the eye back in\n",
    "                                                      # Cu - I + I = Cu\n",
    "            X[u] = spsolve(yTy + yTCuIY + lambda_eye, yTCupu) \n",
    "            # Solve for Xu = ((yTy + yT(Cu-I)Y + lambda*I)^-1)yTCuPu, equation 4 from the paper  \n",
    "        # Begin iteration to solve for Y based on fixed X \n",
    "        for i in range(num_item):\n",
    "            conf_samp = conf[:,i].T.toarray() # transpose to get it in row format and convert to dense\n",
    "            pref = conf_samp.copy()\n",
    "            pref[pref != 0] = 1 # Create binarized preference vector\n",
    "            CiI = sparse.diags(conf_samp, [0]) # Get Ci - I term, don't need to subtract 1 since we never added it\n",
    "            xTCiIX = X.T.dot(CiI).dot(X) # This is the xT(Cu-I)X term\n",
    "            xTCiPi = X.T.dot(CiI + X_eye).dot(pref.T) # This is the xTCiPi term\n",
    "            Y[i] = spsolve(xTx + xTCiIX + lambda_eye, xTCiPi)\n",
    "            # Solve for Yi = ((xTx + xT(Cu-I)X) + lambda*I)^-1)xTCiPi, equation 5 from the paper\n",
    "    # End iterations\n",
    "    return X, Y.T # Transpose at the end to make up for not being transposed at the beginning. \n",
    "                         # Y needs to be rank x n. Keep these as separate matrices for scale reasons. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "user_vecs, item_vecs = implicit_weighted_ALS(product_train, lambda_val = 0.1, alpha = 15, iterations = 1,\n",
    "                                            rank_size = 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([  1.32236293e-05,   8.72394397e-06,   1.24393145e-05,\n",
       "        -5.50696131e-05,   2.72019200e-05])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# For first user, looking at items vectors (user dot items)\n",
    "user_vecs[0,:].dot(item_vecs).toarray()[0,:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can do the same by utilizing Ben Frederickson code at Flipboard, he came out with a version of ALS for Python utilizing Cython and parallelizing the code among threads."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import implicit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "This method is deprecated. Please use the AlternatingLeastSquares class instead\n"
     ]
    }
   ],
   "source": [
    "alpha = 15\n",
    "user_vecs, item_vecs = implicit.alternating_least_squares((product_train*alpha).astype('double'), \n",
    "                                                          factors=20, \n",
    "                                                          regularization = 0.1, \n",
    "                                                         iterations = 50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluating the Recommender System"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn import metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def auc_score(predictions, test):\n",
    "    '''\n",
    "    This simple function will output the area under the curve using sklearn's metrics. \n",
    "    \n",
    "    parameters:\n",
    "    \n",
    "    - predictions: your prediction output\n",
    "    \n",
    "    - test: the actual target result you are comparing to\n",
    "    \n",
    "    returns:\n",
    "    \n",
    "    - AUC (area under the Receiver Operating Characterisic curve)\n",
    "    '''\n",
    "    fpr, tpr, thresholds = metrics.roc_curve(test, predictions)\n",
    "    return metrics.auc(fpr, tpr)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def calc_mean_auc(training_set, altered_users, predictions, test_set):\n",
    "    '''\n",
    "    This function will calculate the mean AUC by user for any user that had their user-item matrix altered. \n",
    "    \n",
    "    parameters:\n",
    "    \n",
    "    training_set - The training set resulting from make_train, where a certain percentage of the original\n",
    "    user/item interactions are reset to zero to hide them from the model \n",
    "    \n",
    "    predictions - The matrix of your predicted ratings for each user/item pair as output from the implicit MF.\n",
    "    These should be stored in a list, with user vectors as item zero and item vectors as item one. \n",
    "    \n",
    "    altered_users - The indices of the users where at least one user/item pair was altered from make_train function\n",
    "    \n",
    "    test_set - The test set constucted earlier from make_train function\n",
    "    \n",
    "    \n",
    "    \n",
    "    returns:\n",
    "    \n",
    "    The mean AUC (area under the Receiver Operator Characteristic curve) of the test set only on user-item interactions\n",
    "    there were originally zero to test ranking ability in addition to the most popular items as a benchmark.\n",
    "    '''\n",
    "    \n",
    "    \n",
    "    store_auc = [] # An empty list to store the AUC for each user that had an item removed from the training set\n",
    "    popularity_auc = [] # To store popular AUC scores\n",
    "    pop_items = np.array(test_set.sum(axis = 0)).reshape(-1) # Get sum of item iteractions to find most popular\n",
    "    item_vecs = predictions[1]\n",
    "    for user in altered_users: # Iterate through each user that had an item altered\n",
    "        training_row = training_set[user,:].toarray().reshape(-1) # Get the training set row\n",
    "        zero_inds = np.where(training_row == 0) # Find where the interaction had not yet occurred\n",
    "        # Get the predicted values based on our user/item vectors\n",
    "        user_vec = predictions[0][user,:]\n",
    "        pred = user_vec.dot(item_vecs).toarray()[0,zero_inds].reshape(-1)\n",
    "        # Get only the items that were originally zero\n",
    "        # Select all ratings from the MF prediction for this user that originally had no iteraction\n",
    "        actual = test_set[user,:].toarray()[0,zero_inds].reshape(-1) \n",
    "        # Select the binarized yes/no interaction pairs from the original full data\n",
    "        # that align with the same pairs in training \n",
    "        pop = pop_items[zero_inds] # Get the item popularity for our chosen items\n",
    "        store_auc.append(auc_score(pred, actual)) # Calculate AUC for the given user and store\n",
    "        popularity_auc.append(auc_score(pop, actual)) # Calculate AUC using most popular and score\n",
    "    # End users iteration\n",
    "    \n",
    "    return float('%.3f'%np.mean(store_auc)), float('%.3f'%np.mean(popularity_auc))  \n",
    "   # Return the mean AUC rounded to three decimal places for both test and popularity benchmark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.843, 0.739)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "calc_mean_auc(product_train, product_users_altered, \n",
    "              [sparse.csr_matrix(user_vecs), sparse.csr_matrix(item_vecs.T)], product_test)\n",
    "# AUC for our recommender system"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that our recommender system beat popularity. Our system has a mean AUC of 0.84, while the popular item benchmark had a lower benchmark of .74. Ideally we would setup a crossvalidation for this to increase the accuracy of the reccommender system. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "customers_arr = np.array(customers) # Array of customer IDs from the ratings matrix\n",
    "products_arr = np.array(products) # Array of product IDs from the ratings matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_items_purchased(customer_id, mf_train, customers_list, products_list, item_lookup):\n",
    "    '''\n",
    "    This just tells me which items have been already purchased by a specific user in the training set. \n",
    "    \n",
    "    parameters: \n",
    "    \n",
    "    customer_id - Input the customer's id number that you want to see prior purchases of at least once\n",
    "    \n",
    "    mf_train - The initial ratings training set used (without weights applied)\n",
    "    \n",
    "    customers_list - The array of customers used in the ratings matrix\n",
    "    \n",
    "    products_list - The array of products used in the ratings matrix\n",
    "    \n",
    "    item_lookup - A simple pandas dataframe of the unique product ID/product descriptions available\n",
    "    \n",
    "    returns:\n",
    "    \n",
    "    A list of item IDs and item descriptions for a particular customer that were already purchased in the training set\n",
    "    '''\n",
    "    cust_ind = np.where(customers_list == customer_id)[0][0] # Returns the index row of our customer id\n",
    "    purchased_ind = mf_train[cust_ind,:].nonzero()[1] # Get column indices of purchased items\n",
    "    prod_codes = products_list[purchased_ind] # Get the stock codes for our purchased items\n",
    "    return item_lookup.loc[item_lookup.product_id.isin(prod_codes)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 156,  159, 2150, 2151, 2155])"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "customers_arr[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>product_id</th>\n",
       "      <th>brand_name</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [product_id, brand_name]\n",
       "Index: []"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_items_purchased(2150, product_train, customers_arr, products_arr, item_lookup)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cust_ind = np.where(customers_arr == 159)[0][0]\n",
    "cust_ind"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.where(customers_arr == 159)[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([  100,   103,   104,   113,   131,   138,   144,   446,   468,\n",
       "         478,   479,   534,   629,   630,   634,   799,   938,   968,\n",
       "        1015,  1120,  1267,  1284,  1494,  1656,  1876,  1878,  1885,\n",
       "        1911,  2117,  2119,  2347,  2753,  3425,  3617,  4064,  4439,\n",
       "        4768,  4814,  4834,  4892,  4986,  4994,  5033,  5133,  5206,\n",
       "        5403,  5417,  5418,  5443,  5476,  5497,  5953,  5959,  6092,\n",
       "        6094,  6156,  6257,  6337,  6375,  6411,  6760,  6800,  7018,\n",
       "        7128,  7138,  7240,  7290,  7298,  7561,  7832,  7932,  7940,\n",
       "        7966,  8120,  8241,  8281,  8410,  8661,  8805,  8911,  9092,\n",
       "        9208,  9248,  9533,  9782, 10305, 10351, 10412, 10498, 10701,\n",
       "       10911, 11317, 11442, 11457, 12029, 12189, 12233, 12273, 12278,\n",
       "       12388, 12529, 12590, 12715, 13224, 13225, 13353, 13355, 13498,\n",
       "       13708, 13712, 13760, 13884, 13886, 13983, 14022, 14025, 14079,\n",
       "       14114, 14284, 14310, 14336, 14431, 14433, 14465, 14563, 15172,\n",
       "       15214, 15272, 15312, 15386, 15391, 15469, 15471, 15488, 15499,\n",
       "       15628, 15778, 15902, 15907, 15908, 15909], dtype=int32)"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "product_train[1,:].nonzero()[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([  343,   347,   348,   362,   431,   442,   475,  1625,  1865,\n",
       "        1964,  1965,  2196,  2394,  2395,  2413,  2921,  3324,  3386,\n",
       "        3496,  3691,  3910,  3934,  4250,  4669,  5410,  5412,  5443,\n",
       "        5519,  6227,  6260,  7287,  8396, 10266, 10782, 12578, 13467,\n",
       "       14626, 14785, 14854, 14989, 15195, 15203, 15273, 15486, 15756,\n",
       "       16080, 16094, 16095, 16132, 16168, 16197, 17621, 17642, 18246,\n",
       "       18260, 18443, 18734, 18956, 19017, 19066, 20370, 20447, 20953,\n",
       "       21154, 21180, 21303, 21376, 21386, 22427, 23530, 23923, 23937,\n",
       "       24048, 24659, 25150, 25238, 25559, 28922, 29176, 29356, 29668,\n",
       "       29990, 30158, 30870, 31394, 33084, 33227, 33468, 33765, 34512,\n",
       "       35525, 37186, 37638, 37688, 42473, 43785, 43988, 44277, 44308,\n",
       "       45293, 46199, 46566, 47078, 49433, 49434, 49764, 49766, 50253,\n",
       "       51153, 51166, 51272, 51497, 51500, 51756, 51835, 51838, 51954,\n",
       "       52032, 52620, 52662, 52717, 53118, 53122, 53209, 53487, 55509,\n",
       "       55642, 55767, 55854, 56338, 56343, 56531, 56533, 56631, 56651,\n",
       "       57120, 57604, 57905, 57913, 57914, 57915])"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prod_codes = products_arr[[  100,   103,   104,   113,   131,   138,   144,   446,   468,\n",
    "         478,   479,   534,   629,   630,   634,   799,   938,   968,\n",
    "        1015,  1120,  1267,  1284,  1494,  1656,  1876,  1878,  1885,\n",
    "        1911,  2117,  2119,  2347,  2753,  3425,  3617,  4064,  4439,\n",
    "        4768,  4814,  4834,  4892,  4986,  4994,  5033,  5133,  5206,\n",
    "        5403,  5417,  5418,  5443,  5476,  5497,  5953,  5959,  6092,\n",
    "        6094,  6156,  6257,  6337,  6375,  6411,  6760,  6800,  7018,\n",
    "        7128,  7138,  7240,  7290,  7298,  7561,  7832,  7932,  7940,\n",
    "        7966,  8120,  8241,  8281,  8410,  8661,  8805,  8911,  9092,\n",
    "        9208,  9248,  9533,  9782, 10305, 10351, 10412, 10498, 10701,\n",
    "       10911, 11317, 11442, 11457, 12029, 12189, 12233, 12273, 12278,\n",
    "       12388, 12529, 12590, 12715, 13224, 13225, 13353, 13355, 13498,\n",
    "       13708, 13712, 13760, 13884, 13886, 13983, 14022, 14025, 14079,\n",
    "       14114, 14284, 14310, 14336, 14431, 14433, 14465, 14563, 15172,\n",
    "       15214, 15272, 15312, 15386, 15391, 15469, 15471, 15488, 15499,\n",
    "       15628, 15778, 15902, 15907, 15908, 15909]] \n",
    "prod_codes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>product_id</th>\n",
       "      <th>brand_name</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [product_id, brand_name]\n",
       "Index: []"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "item_lookup.loc[item_lookup.product_id.isin([  343,   347,   348,   362,   431,   442,   475,  1625,  1865,\n",
    "        1964,  1965,  2196,  2394,  2395,  2413,  2921,  3324,  3386,\n",
    "        3496,  3691,  3910,  3934,  4250,  4669,  5410,  5412,  5443,\n",
    "        5519,  6227,  6260,  7287,  8396, 10266, 10782, 12578, 13467,\n",
    "       14626, 14785, 14854, 14989, 15195, 15203, 15273, 15486, 15756,\n",
    "       16080, 16094, 16095, 16132, 16168, 16197, 17621, 17642, 18246,\n",
    "       18260, 18443, 18734, 18956, 19017, 19066, 20370, 20447, 20953,\n",
    "       21154, 21180, 21303, 21376, 21386, 22427, 23530, 23923, 23937,\n",
    "       24048, 24659, 25150, 25238, 25559, 28922, 29176, 29356, 29668,\n",
    "       29990, 30158, 30870, 31394, 33084, 33227, 33468, 33765, 34512,\n",
    "       35525, 37186, 37638, 37688, 42473, 43785, 43988, 44277, 44308,\n",
    "       45293, 46199, 46566, 47078, 49433, 49434, 49764, 49766, 50253,\n",
    "       51153, 51166, 51272, 51497, 51500, 51756, 51835, 51838, 51954,\n",
    "       52032, 52620, 52662, 52717, 53118, 53122, 53209, 53487, 55509,\n",
    "       55642, 55767, 55854, 56338, 56343, 56531, 56533, 56631, 56651,\n",
    "       57120, 57604, 57905, 57913, 57914, 57915])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>product_id</th>\n",
       "      <th>brand_name</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [product_id, brand_name]\n",
       "Index: []"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "item_lookup[item_lookup.product_id == 343]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
